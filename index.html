
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Scaling Robot Learning with Semantically Imagined Experience</title>

    <meta name="description" content="RT-1: Robotics Transformer">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

    <meta property="og:image" content="https://diffusion-rosie.github.io/img/rosie-teaser.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://diffusion-rosie.github.io/"/>
    <meta property="og:title" content="Project Website for ROSIE" />
    <meta property="og:description" content="Project page for Scaling Robot Learning with Semantically Imagined Experience." />

    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Project Website for ROSIE" />
    <meta name="twitter:description" content="Project page for Scaling Robot Learning with Semantically Imagined Experience." />
    <meta name="twitter:image" content="https://diffusion-rosie.github.io/img/rosie-teaser.png" />


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-52J0PM8XKV"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-52J0PM8XKV');
</script>
	
    <style>
        .nav-pills {
          position: relative;
          display: inline;
        }
        .imtip {
          position: absolute;
          top: 0;
          left: 0;
        }
    </style>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <strong><font size="+3">Scaling Robot Learning with Semantically Imagined Experience</font></strong>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                <br>
			
			<li>Tianhe Yu </li> <li> Ted Xiao </li> <li> Austin Stone </li> <li> Jonathan Tompson </li> <br>
			<li>Anthony Brohan </li> <li> Su Wang </li> <li> Jaspiar Singh </li> <li> Clayton Tan </li> <li> Dee M </li> <br>
			<li> Jodilyn Peralta </li>  <li>Brian Ichter</li> <li> Karol Hausman</li> <li> Fei Xia</li>
		<br><br>
                    <a href="http://g.co/robotics">
                    <image src="img/rng-logo.png" height="37px"> </a>
                    <a href="https://research.google/teams/brain/">   
                    <img src="img/google-research-logo.png" height="25px"> </a>
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="#">
                            <image src="img/paper.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>

                        <li>
                            <a href="https://www.youtube.com/watch?v=9XoABe-cCTU">
                            <image src="img/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
<!--                         <li>
                            <a href="http://ai.googleblog.com/2022/12/rt-1-robotics-transformer-for-real.html">
                            <image src="img/google-ai-blog-small.png" height="60px">
                                <h4><strong>Blogpost</strong></h4>
                            </a>
                        </li>
                         <li>
                            <a href="https://github.com/google-research/robotics_transformer">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>                   
                        </li>  -->
                    </ul>
                </div>
        </div>


   
        

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
             
                <p style="text-align:center;">
        	    	<video id="v0" width="100%" playsinline autoplay muted loop controls>
                       <source src="videos/coke_compressed.mp4" type="video/mp4">
                   </video>
                   We propose using text-guided diffusion models for data augmentation within the sphere of robot learning. These augmentations can produce highly convincing images suitable for learning downstream tasks. As demonstrated in the video, we can augment the background to virtually place the robot in arbitrary backgrounds guided by natural language.
                </p>

                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
Recent advances in robot learning have shown promise in enabling robots to perform a variety of manipulation tasks and generalize to novel scenarios. 
One of the key contributing factors to this progress is the scale of robot data used to train the models.
To obtain large-scale datasets, prior approaches have relied on either demonstrations requiring high human involvement or engineering-heavy autonomous data collection schemes, both of which being challenging in scaling up the space of new tasks and skills needed for building generalist robots. 
To mitigate this issue, we propose to take an alternative route and leverage text-to-image foundation models widely used in computer vision and natural language processing to obtain meaningful data for robot learning without requiring additional robot data. 
Specifically, we make use of the state of the art text-to-image diffusion models and perform aggressive data augmentation on top of our existing robotic manipulation datasets via inpainting of various unseen objects for manipulation, backgrounds, and distractors with pure text guidance. 
Through extensive real-world experiments, we show that manipulation policies trained on the augmented data are able to solve completely unseen tasks with new objects and can behave more robustly w.r.t. novel distractors. In addition, we also find that we can improve the robustness and generalization of high-level robot learning tasks such as success detection through training with the diffusion-based data augmentation.
		</p>
<!--              <p style="text-align:center;">
        	    	<video id="v0" width="100%" playsinline autoplay muted loop>
                       <source src="img/RT1-video.mp4" type="video/mp4">
                   </video>
                   RT-1 shows better performance and generalization thanks to its ability to absorb a large amount of diverse data, including robot trajectories with multiple tasks, objects and environments. Baseline approaches exhibit limited ability to fully utilize large datasets.
                </p> -->
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video 
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/9XoABe-cCTU" allowfullscreen="" style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
             
        </div>




        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Introduction 
                </h3>

                <p>
                    Though recent progress in robotic learning has shown the ability to learn impressive tasks,
it has largely been limited to domains with few tasks and constrained environments. One of the
fundamental reasons for these limitations is the lack of diverse data that covers not only a large
variety of motor skills, but also a variety of objects and visual domains. This becomes apparent
by observing more recent trends in robot learning research – when scaled to larger, more diverse
datasets, current robotic learning algorithms have demonstrated promising signs towards more robust
and performant robotic systems. However, this promise comes with an arduous challenge: it is
extremely difficult to significantly scale-up varied, real-world data collected by robots as it requires
either engineering-heavy autonomous schemes such as scripted policies or laborious human
teleoperations. To put it into perspective, it took 17 months and 13 robots to collect 130k
demonstrations in RT-1. While some works have proposed a potential solution to this conundrum by
generating simulated data to satisfy these robot data needs, they come with their own set of challenges
such as generating diverse and accurate enough simulations  or solving sim-to-real transfer.
Can we find other ways to synthetically generate realistic diverse data without requiring realistic
simulations or actual data collection on real robots?
                </p>

                <video id="v0" width="100%" playsinline autoplay muted loop controls>
                    <source src="videos/intro.mp4" type="video/mp4">
                </video>

<p>
        Recent text-to-image diffusion models, like DALL-E 2 or Imagen allow us to 
        move beyond traditional data augmentation. First, these models can augment 
        semantic aspects through natural language input. Second, they can generate images 
        without ever having seen real-world examples. Third, they can modify an image without 
        changing any meaningful parts.
    </p>
    <p>
        These capabilities all allow us to create realistic scenes with objects that the robot has never interacted with or backgrounds that it's never seen before. This means that we can take internet-scale data and distill it into robot experience. ROSIE (Robot Learning with Semantically Imaged Experience), is the data augmentation approach proposed in this paper. It follows these steps: it parses instructions from humans and identifies portions of scenes which need to change. It then uses inpainting to modify only those areas, leaving the other elements intact. This results in new instructions and semantically-labelled data which teach the robot new tasks.
    </p>
    <p>
        ROSIE was evaluated on a large set of robotic data. The data was used to train a robot policy, which was then able to perform new tasks and it also improved the ability to detect success, particularly in out-of-distribution (OOD) scenarios.
    </p>
</div>
</div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Approach 
                </h3>

      
                <p style="text-align:center;">
                    <img data-enlargable src="img/approach.png" class="img-responsive">
                </p>

                
                The proposed architecture of ROSIE. First, we localize the augmentation region with open
vocabulary segmentation model. Second, we run Imagen Editor to perform text-guided image editing.
Finally, we use the augmented data to train an RT-1 manipulation policy. Concretely, we explain
ROSIE using the example shown in the figure as follows. We take the original episode with the
instruction “place coke can into top drawer” and the goal is to add distractors in the opened drawer to
improve the robustness of the policy. For each image in the episode, we detect the masks of the open
drawer, the robot arm, and the coke can using our first step. We obtain the mask of the target region
to add the distractor via subtracting the masks of the robot arm and the coke can that is picked up
from the mask of the open drawer. Then, we generate our augmentation proposal leveraging LLMs. We run Imagen Editor with the augmentation text and the selected mask
to generate a coke can in the drawer. We combine both the original episodes
and the augmented episodes and perform policy training using multi-task imitation learning.

<p style="text-align:center;">
    <img data-enlargable src="img/example_generation.png" class="img-responsive">
</p>

We show visualizations of the episodes generated by ROSIE where we replace the regular
tabletop in front of the robot with a dish rack, a marble sink and a wooden counter, which never
appears in the training dataset.


<p style="text-align:center;">
    <video id="v0" width="100%" playsinline autoplay muted loop controls>
       <source src="videos/chips_compressed.mp4" type="video/mp4">
   </video>
   </p>

We can also pinpoint the augmentation to a small region of the image, as is shown in the video, where we 
change the object in the drawer. Furthermore, we are able to augment in-hand objects, as is shown in the last
part of this video.

                </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results 
                </h3>

                Concretely, we ask 3 research questions. 
<ul>
                <li>RQ1: Can we leverage semantic-aware augmentation to learn completely new skills only
seen through diffusion models?</li>
<li>RQ2: Can we leverage semantic-aware augmentation to make our policy more robust to
visual distractors?</li>
<li>RQ3: Can we leverage semantic-aware augmentation to bootstrap high-level embodied
reasoning such as success detection?</li>
</ul>

<p style="text-align:center;">
    <img src="img/result_table.png" class="img-responsive">
</p>

                In this table we show the full Experimental Results for ROSIE. The blue shaded results correspond to RQ1 and the
orange shaded results correspond to RQ2. For each task family from top to the bottom, we performed
evaluations with 50, 20, 16, 10, 80, 40, and 27 episodes respectively (243 episodes in total). ROSIE
outperforms NoAug (pre-trained RT-1 policy) and InstructionAug (fine-tuned RT-1 policy with
instruction augmentation) in both categories, suggesting that ROSIE can significantly improve
the generalization to novel tasks and robustness w.r.t. different distractors. For RQ3 results please refer to our paper.
            

<p style="text-align:center;">
    <video id="v0" width="100%" playsinline autoplay muted loop controls>
       <source src="videos/sink2.mp4" type="video/mp4">
   </video>
   </p>

   We show an episode augmented by ROSIE (left) where ROSIE inpaints the metal sink
   onto the top drawer of the counter and a rollout of policy trained with both the original episodes
   and the augmented episodes in a real kitchen with a metal sink. The policy successfully performs
   the task “place pepsi can into sink” even if it is not trained on real data with sink before, suggesting
   that leveraging the prior of the diffusion models trained with internet-scale data is able to improve
   generalization of robotic learning in the real world.


   <p style="text-align:center;">
    <video id="v0" width="100%" playsinline autoplay muted loop controls>
       <source src="videos/cloth2.mp4" type="video/mp4">
   </video>
   </p>

   We show an augmented by ROSIE (left), where we inpaint the green rice chip bag into microfiber cloth 
   of different colors, and the policies trained on the mixed dataset gets rolled out on the right. RT-1 trained
   on mixed data has a higher chance of picking up the microfiber cloths.


    </div>
</div>

<div class="row">
    <div class="col-md-8 col-md-offset-2">
        <h3>
            Demo 
        </h3>
<div class="compositional">
    <div class="image">
        <div class="col-md-6">

      <img src="./img/imagen/original.gif">
      </div>
      <div class="col-md-6">

      <img src="./img/imagen/blue_solid_none.gif" id="compositional_animals_img">
      </div>
    </div>
    <div class="text" id="compositional_animals">
      <h3>Click on a word below and Imagen!</h3>
      A robot arm picking up a
      <p class="selectable left"><span class="selected">blue</span> <span>red</span></p> 
      <p class="selectable left"><span class="selected">solid</span> <span>striped</span> microfiber cloth</p>
      <p class="selectable left">from a <span >counter</span> <span>sink</span> <span class="selected">none</span></p>
      <p class="selectable left">full of <span class="selected">bowls</span> <span>fruits</span></p>

      <!-- <p class="selectable left"><span class="selected">A photo of a</span> <span>An oil painting of a</span></p><p>
      </p>
      <p class="selectable left"><span class="selected">fuzzy panda</span> <span class="">British Shorthair cat</span> <span class="">Persian cat</span> <span>Shiba Inu dog</span> <span class="">raccoon</span></p><p>
      </p>
      <p class="selectable left"><span class="">wearing a cowboy hat and</span> <span class="selected">wearing a sunglasses and</span></p>
      <p class="selectable left"><span class="">red shirt</span> <span class="selected">black leather jacket</span></p>
      <p class="selectable left"><span class="">playing a guitar</span> <span class="">riding a bike</span> <span class="selected">skateboarding</span></p>
      <p class="selectable left"><span class="">in a garden.</span> <span class="selected">on a beach.</span> <span class="">on top of a mountain.</span></p> -->
    </div>
  </div>
  
  </div>
  </div>

         <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation 
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@inproceedings{yu2023scaling,
    title={Scaling Robot Learning with Semantically Imagined Experience},
    author={Tianhe Yu and Ted Xiao and Austin Stone and Jonathan Tompson and Anthony Brohan and Su Wang and Jaspiar Singh and Clayton Tan and Dee M and Jodilyn Peralta and Brian Ichter and Karol Hausman and Fei Xia},
    booktitle={arXiv preprint arXiv:2212.06817},
    year={2023}
}</textarea>
                </div>
            </div>
             
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Concurrent Works 
                </h3>

            <br>
                Check out other awesome work that also leverages pretrained generative models for robot learning!
                <br>
                <a href="https://genaug.github.io/">GenAug: Retargeting behaviors to unseen situations via Generative Augmentation. by Chen et al.</a> shows using generative models to create “augmented” RGBD images for entirely different and realistic environments, and retarget behaviors to new scenarios.
            
                <br>
                <a href="https://arxiv.org/abs/2212.05711">CACTI: A Framework for Scalable Multi-Task Multi-Scene Visual Imitation Learning by Mandi et al.</a> shows using generative models like stable diffusion to add distractors of the scene can robustify the multi-task policy learning.
            
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                    The authors would like to thank Pierre Sermanet for early discussions, and Alex Irpan, Quan Vuong, and Vincent Vanhoucke for feedbacks on an early draft.

                    <br><br>
                The website template was borrowed from <a href="http://jonbarron.info/">Jon Barron</a>.
                </p>
            </div>
        </div>
    </div>
</body>
<script type="text/javascript">
    compositionalAnimalsManualClicked = false;
    compositionalMadeOfManualClicked = false;
    var root = '.';
    if (window.location.host.includes('.appspot.com') || window.location.host.includes('research.google')) {
            root = 'https://gweb-research-imagen.web.app';
    }

    function updateComposition(name) {
      var compositionSpans = $(name + " p span.selected").map(function() {
        return $(this).text();
      }).get();
      var compositionText = "";
      for (var compositionSpansIdx = 0; compositionSpansIdx < compositionSpans.length; ++compositionSpansIdx) {
        if (compositionSpansIdx > 0) {
          compositionText += "_";
        }
        compositionText += compositionSpans[compositionSpansIdx];
        if (compositionSpans[compositionSpansIdx] == "none") {
            break;
        }
      }
      console.info(compositionText);
      if (name == "#compositional_madeof") {
        //randImgIdx = Math.floor(Math.random() * 8) + 1;
        //$(name + "_img").attr("src", "./img/imagen/" + compositionText + ".gif");
      } else if (name == "#compositional_animals") {
        randImgIdx = Math.floor(Math.random() * 4);
        $(name + "_img").attr("src",  "./img/imagen/" + compositionText + ".gif");
      }
    }

    function randomComposition(name) {
      if (name == "#compositional_animals" && compositionalAnimalsManualClicked == true) {
        compositionalAnimalsManualClicked = false;
        //setTimeout(randomComposition, 4000, name);
        return;
      } else if (name == "#compositional_madeof" &&
                 compositionalMadeOfManualClicked == true) {
        compositionalMadeOfManualClicked = false;
        //setTimeout(randomComposition, 4000, name);
        return;
      }

      var compositionPs = $(name + " p.selectable").toArray();
      var compositionOptions = [];
      for (var i = 0; i < compositionPs.length; ++i) {
        var compositionSpanSelected = $(compositionPs[i]).children("span.selected");
        var compositionSpans = $(compositionPs[i]).children("span").not("span.selected").toArray();
        for (var j = 0; j < compositionSpans.length; ++j) {
          compositionOption = {
            "p": compositionPs[i],
            "span_selected": compositionSpanSelected,
            "span_option": $(compositionSpans[j])
          };
          compositionOptions.push(compositionOption);
        }
      }

      randomCompositionOptionIdx = Math.floor(Math.random() * compositionOptions.length);
      randomCompositionOption = compositionOptions[randomCompositionOptionIdx];

      randomCompositionOption.span_selected.toggleClass("selected");
      randomCompositionOption.span_option.toggleClass("selected");

      updateComposition(name);
      //setTimeout(randomComposition, 4000, name);
    }

    $("#compositional_animals p.selectable span").click(function() {
      $(this).siblings().removeClass("selected");
      $(this).addClass("selected");
      updateComposition("#compositional_animals");
      compositionalAnimalsManualClicked = true;
    });

    $("#compositional_madeof p.selectable span").click(function() {
      $(this).siblings().removeClass("selected");
      $(this).addClass("selected");
      updateComposition("#compositional_madeof");
      compositionalMadeOfManualClicked = true;
    });

    //setTimeout(randomComposition, 2000, "#compositional_animals");
    //setTimeout(randomComposition, 4000, "#compositional_madeof");
  </script>

<script>
    $('img[data-enlargable]').addClass('img-enlargable').click(function(){
var src = $(this).attr('src');
$('<div>').css({
background: 'RGBA(0,0,0,.5) url('+src+') no-repeat center',
backgroundSize: 'contain',
width:'100%', height:'100%',
position:'fixed',
zIndex:'10000',
top:'0', left:'0',
cursor: 'zoom-out'
}).click(function(){
$(this).remove();
}).appendTo('body');
});
</script>

</html>
