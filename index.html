
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Scaling Robot Learning with Semantically Imagined Experience</title>

    <meta name="description" content="RT-1: Robotics Transformer">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

    <meta property="og:image" content="https://diffusion-rosie.github.io/img/rosie-teaser.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://diffusion-rosie.github.io/"/>
    <meta property="og:title" content="Project Website for ROSIE" />
    <meta property="og:description" content="Project page for Scaling Robot Learning with Semantically Imagined Experience." />

    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Project Website for ROSIE" />
    <meta name="twitter:description" content="Project page for Scaling Robot Learning with Semantically Imagined Experience." />
    <meta name="twitter:image" content="https://diffusion-rosie.github.io/img/rosie-teaser.png" />


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-52J0PM8XKV"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-52J0PM8XKV');
</script>
	
    <style>
        .nav-pills {
          position: relative;
          display: inline;
        }
        .imtip {
          position: absolute;
          top: 0;
          left: 0;
        }
    </style>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <strong><font size="+3">Scaling Robot Learning with Semantically Imagined Experience</font></strong>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                <br>
			
			<li>Tianhe Yu </li> <li> Ted Xiao </li> <li> Austin Stone </li> <li> Jonathan Tompson </li> <br>
			<li>Anthony Brohan </li> <li> Su Wang </li> <li> Jaspiar Singh </li> <li> Clayton Tan </li> <li> Dee M </li> <br>
			<li> Jodilyn Peralta </li>  <li>Brian Ichter</li> <li> Karol Hausman</li> <li> Fei Xia</li>
		<br><br>
                    <a href="http://g.co/robotics">
                    <image src="img/rng-logo.png" height="37px"> </a>
                    <a href="https://research.google/teams/brain/">   
                    <img src="img/google-research-logo.png" height="25px"> </a>
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="#">
                            <image src="img/paper.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>

                        <li>
                            <a href="https://www.youtube.com/watch?v=9XoABe-cCTU">
                            <image src="img/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
<!--                         <li>
                            <a href="http://ai.googleblog.com/2022/12/rt-1-robotics-transformer-for-real.html">
                            <image src="img/google-ai-blog-small.png" height="60px">
                                <h4><strong>Blogpost</strong></h4>
                            </a>
                        </li>
                         <li>
                            <a href="https://github.com/google-research/robotics_transformer">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>                   
                        </li>  -->
                    </ul>
                </div>
        </div>


   
        

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
             
                <p style="text-align:center;">
        	    	<video id="v0" width="100%" playsinline autoplay muted loop>
                       <source src="videos/coke_compressed.mp4" type="video/mp4">
                   </video>
                   We propose using text-guided diffusion models for data augmentation within the sphere of robot learning. These augmentations can produce highly convincing images suitable for learning downstream tasks. As demonstrated in the video, we can augment the background to virtually place the robot in arbitrary backgrounds guided by natural language.
                </p>

                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
Recent advances in robot learning have shown promise in enabling robots to perform a variety of manipulation tasks and generalize to novel scenarios. 
One of the key contributing factors to this progress is the scale of robot data used to train the models.
To obtain large-scale datasets, prior approaches have relied on either demonstrations requiring high human involvement or engineering-heavy autonomous data collection schemes, both of which being challenging in scaling up the space of new tasks and skills needed for building generalist robots. 
To mitigate this issue, we propose to take an alternative route and leverage text-to-image foundation models widely used in computer vision and natural language processing to obtain meaningful data for robot learning without requiring additional robot data. 
Specifically, we make use of the state of the art text-to-image diffusion models and perform aggressive data augmentation on top of our existing robotic manipulation datasets via inpainting of various unseen objects for manipulation, backgrounds, and distractors with pure text guidance. 
Through extensive real-world experiments, we show that manipulation policies trained on the augmented data are able to solve completely unseen tasks with new objects and can behave more robustly w.r.t. novel distractors. In addition, we also find that we can improve the robustness and generalization of high-level robot learning tasks such as success detection through training with the diffusion-based data augmentation.
		</p>
<!--              <p style="text-align:center;">
        	    	<video id="v0" width="100%" playsinline autoplay muted loop>
                       <source src="img/RT1-video.mp4" type="video/mp4">
                   </video>
                   RT-1 shows better performance and generalization thanks to its ability to absorb a large amount of diverse data, including robot trajectories with multiple tasks, objects and environments. Baseline approaches exhibit limited ability to fully utilize large datasets.
                </p> -->
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video 
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/9XoABe-cCTU" allowfullscreen="" style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
             
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Approach 
                </h3>

      
        
        
                <p style="text-align:center;">
                    <img src="img/approach.png" class="img-responsive">
                </p>
                
                The proposed architecture of ROSIE. First, we localize the augmentation region with open
vocabulary segmentation model. Second, we run Imagen Editor to perform text-guided image editing.
Finally, we use the augmented data to train an RT-1 manipulation policy. Concretely, we explain
ROSIE using the example shown in the figure as follows. We take the original episode with the
instruction “place coke can into top drawer” and the goal is to add distractors in the opened drawer to
improve the robustness of the policy. For each image in the episode, we detect the masks of the open
drawer, the robot arm, and the coke can using our first step. We obtain the mask of the target region
to add the distractor via subtracting the masks of the robot arm and the coke can that is picked up
from the mask of the open drawer. Then, we generate our augmentation proposal leveraging LLMs. We run Imagen Editor with the augmentation text and the selected mask
to generate a coke can in the drawer. We combine both the original episodes
and the augmented episodes and perform policy training using multi-task imitation learning.

<p style="text-align:center;">
    <img src="img/example_generation.png" class="img-responsive">
</p>

We show visualizations of the episodes generated by ROSIE where we replace the regular
tabletop in front of the robot with a dish rack, a marble sink and a wooden counter, which never
appears in the training dataset.


<p style="text-align:center;">
    <video id="v0" width="100%" playsinline autoplay muted loop>
       <source src="videos/chips_compressed.mp4" type="video/mp4">
   </video>
   </p>

We can also pinpoint the augmentation to a small region of the image, as is shown in the video, where we 
change the object in the drawer. Furthermore, we are able to augment in-hand objects, as is shown in the last
part of this video.

                </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results 
                </h3>

                Concretely, we ask 3 research questions. 
<ul>
                <li>RQ1: Can we leverage semantic-aware augmentation to learn completely new skills only
seen through diffusion models?</li>
<li>RQ2: Can we leverage semantic-aware augmentation to make our policy more robust to
visual distractors?</li>
<li>RQ3: Can we leverage semantic-aware augmentation to bootstrap high-level embodied
reasoning such as success detection?</li>
</ul>

<p style="text-align:center;">
    <img src="img/result_table.png" class="img-responsive">
</p>

                In this table we show the full Experimental Results for ROSIE. The blue shaded results correspond to RQ1 and the
orange shaded results correspond to RQ2. For each task family from top to the bottom, we performed
evaluations with 50, 20, 16, 10, 80, 40, and 27 episodes respectively (243 episodes in total). ROSIE
outperforms NoAug (pre-trained RT-1 policy) and InstructionAug (fine-tuned RT-1 policy with
instruction augmentation) in both categories, suggesting that ROSIE can significantly improve
the generalization to novel tasks and robustness w.r.t. different distractors. For RQ3 results please refer to our paper.
            

<p style="text-align:center;">
    <video id="v0" width="100%" playsinline autoplay muted loop controls>
       <source src="videos/sink2.mp4" type="video/mp4">
   </video>
   </p>

   We show an episode augmented by ROSIE (left) where ROSIE inpaints the metal sink
   onto the top drawer of the counter and a rollout of policy trained with both the original episodes
   and the augmented episodes in a real kitchen with a metal sink. The policy successfully performs
   the task “place pepsi can into sink” even if it is not trained on real data with sink before, suggesting
   that leveraging the prior of the diffusion models trained with internet-scale data is able to improve
   generalization of robotic learning in the real world.


   <p style="text-align:center;">
    <video id="v0" width="100%" playsinline autoplay muted loop controls>
       <source src="videos/cloth2.mp4" type="video/mp4">
   </video>
   </p>

   We show an augmented by ROSIE (left), where we inpaint the green rice chip bag into microfiber cloth 
   of different colors, and the policies trained on the mixed dataset gets rolled out on the right. RT-1 trained
   on mixed data has a higher chance of picking up the microfiber cloths.


</div>
        </div>




         <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation 
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@inproceedings{yu2023scaling,
    title={Scaling Robot Learning with Semantically Imagined Experience},
    author={Tianhe Yu and Ted Xiao and Austin Stone and Jonathan Tompson and Anthony Brohan and Su Wang and Jaspiar Singh and Clayton Tan and Dee M and Jodilyn Peralta and Brian Ichter and Karol Hausman and Fei Xia},
    booktitle={arXiv preprint arXiv:2212.06817},
    year={2023}
}</textarea>
                </div>
            </div>
             
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Concurrent Works 
                </h3>

            <br>
                Check out other awesome work that also leverages pretrained generative models for robot learning!
                <br>
                <a href="https://genaug.github.io/">GenAug: Retargeting behaviors to unseen situations via Generative Augmentation. by Chen et al.</a> shows using generative models to create “augmented” RGBD images for entirely different and realistic environments, and retarget behaviors to new scenarios.
            
                <br>
                <a href="https://arxiv.org/abs/2212.05711">CACTI: A Framework for Scalable Multi-Task Multi-Scene Visual Imitation Learning by Mandi et al.</a> shows using generative models like stable diffusion to add distractors of the scene can robustify the multi-task policy learning.
            
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                    The authors would like to thank Pierre Sermanet for early discussions, and Alex Irpan, Quan Vuong, and Vincent Vanhoucke for feedbacks on an early draft.

                    <br><br>
                The website template was borrowed from <a href="http://jonbarron.info/">Jon Barron</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
